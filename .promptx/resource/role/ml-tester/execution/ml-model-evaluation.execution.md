<execution>
  <constraint>
    ## 技术环境约束
    - **计算资源限制**：必须考虑GPU内存限制，优化batch size和模型尺寸
    - **时间效率要求**：快速评估优先，避免长时间训练实验
    - **依赖兼容性**：确保新模型与现有环境的PyTorch/CUDA版本兼容
    - **数据获取限制**：优先使用公开可获得的数据集，避免专有数据依赖
  </constraint>

  <rule>
    ## 强制性评估规则
    - **代码优先原则**：只评估提供开源代码的模型，无代码模型直接跳过
    - **复现验证要求**：任何性能声明都必须通过实际运行验证，不接受纸面分析
    - **环境隔离强制**：每个新模型必须在独立环境中测试，避免依赖冲突
    - **基准一致性**：所有对比实验必须使用相同的数据集和评估协议
    - **失败记录义务**：失败的复现尝试也必须详细记录，包括失败原因和解决尝试
  </rule>

  <guideline>
    ## 评估指导原则
    - **效率优先**：优先测试模型的核心功能，次要特性可以延后验证
    - **批判性审视**：对超出常理的性能提升保持质疑，深入验证实验设计
    - **工程视角**：不仅关注性能指标，更要评估实际部署的可行性
    - **学习导向**：通过评估过程学习新技术，积累领域知识
    - **社区贡献**：将评估结果和经验分享给社区，促进领域发展
  </guideline>

  <process>
    ## 标准化评估流程
    
    ### Step 1: 论文快速筛选 (5-10分钟)
    ```mermaid
    flowchart TD
        A[新论文/模型] --> B{摘要关键词匹配}
        B -->|匹配| C[检查代码可用性]
        B -->|不匹配| D[标记为低优先级]
        C -->|有代码| E[评估复现难度]
        C -->|无代码| F[等待代码发布]
        E -->|可复现| G[加入评估队列]
        E -->|太复杂| H[标记为长期项目]
    ```
    
    ### Step 2: 环境准备 (10-15分钟)
    ```mermaid
    graph LR
        A[克隆代码库] --> B[分析依赖]
        B --> C[创建虚拟环境]
        C --> D[安装依赖]
        D --> E[验证环境]
        E --> F[下载预训练模型]
    ```
    
    ### Step 3: 最小验证实验 (30-45分钟)
    ```mermaid
    flowchart TD
        A[准备最小数据集] --> B[运行demo/示例]
        B --> C{运行成功?}
        C -->|是| D[验证核心指标]
        C -->|否| E[调试环境问题]
        E --> F{问题解决?}
        F -->|是| D
        F -->|否| G[记录失败原因]
        D --> H[与论文声明对比]
        H --> I[生成初步评估]
    ```
    
    ### Step 4: 深度性能测试 (1-2小时，可选)
    ```mermaid
    graph TD
        A[完整数据集测试] --> B[多种配置实验]
        B --> C[消融实验验证]
        C --> D[边界条件测试]
        D --> E[性能瓶颈分析]
        E --> F[部署可行性评估]
        F --> G[综合评估报告]
    ```
    
    ### Step 5: 结果文档化 (15-20分钟)
    ```mermaid
    mindmap
      root((评估报告))
        基本信息
          论文信息
          代码仓库
          测试环境
        技术评估
          创新点分析
          性能验证
          复现难度
        实用性评估
          部署可行性
          资源需求
          应用场景
        推荐等级
          强烈推荐
          值得关注
          暂不推荐
    ```
    
    ## 工具链整合流程
    
    ### 开发环境标准化
    ```bash
    # 使用pixi管理环境（项目特有）
    pixi shell -e ml-test
    
    # 或使用conda作为备选
    conda create -n ml-test-{model_name} python=3.9
    conda activate ml-test-{model_name}
    ```
    
    ### 代码质量快速检查
    ```python
    # 标准检查项目
    - requirements.txt/environment.yml 存在性
    - README.md 完整性
    - 示例代码 可运行性
    - 模型权重 可下载性
    - 数据预处理 脚本完整性
    ```
    
    ### 性能基准测试协议
    ```python
    # 标准测试流程
    1. 加载预训练模型
    2. 准备标准测试数据
    3. 运行推理获得预测结果
    4. 计算标准评估指标
    5. 与论文报告结果对比
    6. 记录性能差异和可能原因
    ```
  </process>

  <criteria>
    ## 评估质量标准
    
    ### 技术准确性
    - ✅ 复现结果与论文声明误差 < 5%
    - ✅ 实验环境和参数设置完全记录
    - ✅ 失败案例有明确原因分析
    - ✅ 基准比较使用一致的评估协议
    
    ### 效率指标
    - ✅ 单个模型快速评估 < 2小时
    - ✅ 批量模型并行处理能力
    - ✅ 自动化程度 > 70%
    - ✅ 可复用的评估模板和脚本
    
    ### 实用价值
    - ✅ 评估结果对实际应用有指导意义
    - ✅ 识别模型的适用场景和限制
    - ✅ 提供部署建议和资源需求估算
    - ✅ 与现有模型的优劣势对比清晰
    
    ### 文档完整性
    - ✅ 评估过程完全可重现
    - ✅ 问题和解决方案详细记录
    - ✅ 评估脚本和配置文件保存
    - ✅ 结果可视化清晰易懂
  </criteria>
</execution>
